{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) data extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def extract_content(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    return soup.get_text()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\shiva\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\shiva\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\shiva\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\shiva\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import re\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Download required NLTK resources\n",
    "nltk.download('punkt')         # For tokenization\n",
    "nltk.download('stopwords')     # For stopwords\n",
    "nltk.download('wordnet')       # For lemmatization\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "# Initialize the lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess_content(text):\n",
    "    text = text.lower() #  Convert to lowercase\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)     # Remove URLs\n",
    "\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text) # Remove non-alphanumeric characters (punctuation, special characters)\n",
    "    tokens = word_tokenize(text, language='english', preserve_line=True)    # Tokenize the text\n",
    "\n",
    "    stop_words = set(stopwords.words('english'))        # Remove stopwords\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]    # Lemmatize tokens\n",
    "    \n",
    "    return tokens\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def generate_important_recommendations(user_keywords, competitor_keywords):\n",
    "    # Join keywords into documents (texts) for TF-IDF processing\n",
    "    user_text = \" \".join(user_keywords)     # list -> 1 string -> 1 doc\n",
    "    competitor_text = \" \".join(competitor_keywords)\n",
    "    \n",
    "    # Use TF-IDF Vectorizer to get importance scores\n",
    "    vectorizer = TfidfVectorizer()      # creating a vectorizer obj\n",
    "    tfidf_matrix = vectorizer.fit_transform([user_text, competitor_text])       # combines the strings -> \n",
    "    # fit leanrs stats that is need for transforming the data,      # transform applies TF-IDF on the data\n",
    "    \n",
    "    # Extract TF-IDF scores for the competitor's keywords\n",
    "    feature_names = vectorizer.get_feature_names_out()          # get the list of unique features learnt by the vectorizer obj durting fit\n",
    "    competitor_tfidf_scores = tfidf_matrix[1].toarray()[0]  # Competitor's TF-IDF row\n",
    "    # torray give [[all the TF-IDF vals]]\n",
    "    \n",
    "    # Create a dictionary of words and their TF-IDF scores for the competitor\n",
    "    tfidf_scores = {feature_names[i]: competitor_tfidf_scores[i] for i in range(len(feature_names))}\n",
    "    \n",
    "    # Sort words by TF-IDF score in descending order (high to low importance)\n",
    "    important_keywords = sorted(tfidf_scores, key=tfidf_scores.get, reverse=True)\n",
    "    \n",
    "    # Generate recommendations for high-importance keywords missing from user content\n",
    "    recommendations = []\n",
    "    cnt = 0\n",
    "    for keyword in important_keywords:\n",
    "        if keyword not in user_keywords:\n",
    "            recommendations.append(f\"Consider adding more content about '{keyword}' to improve relevance.\")\n",
    "            cnt+=1\n",
    "        if cnt == 10:\n",
    "            break\n",
    "    \n",
    "    return recommendations\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommendations:\n",
      "Consider adding more content about 'expert' to improve relevance.\n",
      "Consider adding more content about 'classexternal' to improve relevance.\n",
      "Consider adding more content about 'classmwparseroutputplicense' to improve relevance.\n",
      "Consider adding more content about 'commonsabrnppbr' to improve relevance.\n",
      "Consider adding more content about 'href' to improve relevance.\n",
      "Consider adding more content about 'noopener' to improve relevance.\n",
      "Consider adding more content about 'noreferrer' to improve relevance.\n",
      "Consider adding more content about 'npdiv' to improve relevance.\n",
      "Consider adding more content about 'relnofollow' to improve relevance.\n",
      "Consider adding more content about 'smallurl' to improve relevance.\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    user_url = \"https://en.wikipedia.org/wiki/Health\"\n",
    "    user_content = extract_content(user_url)\n",
    "    user_keywords = preprocess_content(user_content)\n",
    "\n",
    "    # Example competitor keywords for testing\n",
    "    competitor_urls = [\"https://www.wikihow.com/Be-Healthy#:~:text=Things%20You%20Should%20Know,based%20on%20your%20physical%20frame).\", \"https://www.wikihow.com/Category:Health\"]\n",
    "    competitor_contents = [extract_content(url) for url in competitor_urls]\n",
    "    \n",
    "    competitor_keywords = []\n",
    "    for content in competitor_contents:\n",
    "        competitor_keywords.extend(preprocess_content(content))\n",
    "    \n",
    "    # Generate recommendations\n",
    "    recommendations = generate_important_recommendations(user_keywords, competitor_keywords)\n",
    "    \n",
    "    # Display recommendations\n",
    "    print(\"Recommendations:\")\n",
    "    for recommendation in recommendations:\n",
    "        print(recommendation)\n",
    "\n",
    "if __name__ == \"__main__\": \n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
